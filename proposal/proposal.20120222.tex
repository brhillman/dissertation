\documentclass{article}
\def\baselinestretch{1.5}
\usepackage{lineno}
\usepackage{fullpage}
\usepackage{natbib}
\author{Benjamin R. Hillman}
\title{On improving the interaction of clouds and radiation in large-scale models}

\begin{document}
\linenumbers
\maketitle
\section{Introduction}
Local heating or cooling of the atmosphere due to the interaction of radiation with the earth-atmosphere system is a fundamental driver of atmospheric circulations and a key term in the global energy budget. Accurate modeling of the interaction of radiation with the atmosphere depends first and foremost on having an accurate description of the state of the atmosphere at the scales required to calculate local fluxes and heating rates. This includes the requirement of an accurate description of both the horizontal and vertical distribution of clouds, which can vary on scales down to kilometers or less.

Large-scale models used to study weather operate with resolutions typically on the order of tens of kilometers. Models used to study climate operate with even coarser resolutions, typically on the order of hundreds of kilometers. The low resolution of these models precludes them from resolving individual cloud elements and explicitly representing the processes important for forming them, such as small-scale turbulence and convection. Instead, these processes are ``parameterized'' in these types of models to provide a statistical representation of the important quantities.

The description of clouds in large-scale models typically includes grid-box means of the cloud occurrence and the liquid and ice cloud condensate \citep[e.g.][]{cam3_description,cam4_description}. Including the grid-box mean cloud occurrence provides a profile of ``partial cloudiness'' in each column of the model, but does not in itself specify how the clouds that make up the mean should be distributed horizontally and vertically within the grid-box. Solving for the radiative fluxes through each layer requires a description of how clouds overlap in the vertical. This vertical structure is often accounted for using simple rules called ``overlap assumptions'' \citep[e.g.][]{collins_2001} that describe how clouds in adjacent vertical layers should overlap. Applying these overlap assumptions, a set of possible binary cloud configurations can be determined from the profiles of partial cloudiness. The radiative fluxes from these different configurations can then be treated separately and appropriately combined to obtain estimates of the grid-box domain-averaged fluxes and heating rates. 

Subgrid-scale variability in the liquid and ice cloud condensate amount is often neglected in large-scale models, and these quantities are typically assumed to be homogeneous in the cloudy portions of each layer in each column \citep[e.g.][]{cam3_description,cam4_description,cam5_description,am2_evaluation,donner_et_al_2011}. Yet, cloud properties can exhibit substantial variability on scales much smaller than the resolutions typical of large-scale models (citations?). \cite{barker_et_al_1999} showed that neglecting this variability can lead to large biases in shortwave fluxes. Moreover, clouds can have more complicated vertical structure than is often represented by simple overlap assumptions (citations?), and \cite{barker_et_al_1999} show that incorrect overlap of clouds can also contribute to large biases in fluxes and heating rates. This emphasizes the need to provide more realistic descriptions of clouds as seen by the radiative transfer solvers in large-scale models that include both horizontal variability in cloud properties and more realistic cloud overlap.

The focus of this study will be to develop, test, and implement a parameterization of subgrid-scale cloud variability and overlap which provides a more comprehensive description of clouds to be used in radiative calculations in large-scale models.

\section{Background}
It has long been recognized that neglecting subgrid-scale variability in plane-parallel homogeneous radiative transfer calculations can lead to biases in computed fluxes and heating rates in large-scale models, and a number of strategies have been proposed and tried to deal with this problem. One strategy which has seen use in global climate models \citep[e.g.][]{am2_evaluation} has been to scale cloud optical properties in some way so that the computed fluxes more closely match those observed. Different scalings have been proposed by \cite{davis_et_al_1990},\cite{cahalan_et_al_1994}, and \cite{cairns_et_al_2000}. However, using only a single scaling with this strategy has been shown to work only for a narrow range of optical depths and solar zenith angles \citep{cahalan_et_al_1994,barker_1996}. The ideal scaling factor has also been shown to be dependent on grid-box size \citep{pomroy_and_illingworth_2000}, spectral region \citep{yu_et_al_1997}, and on geographic region \citep{oreopoulos_and_cahalan_2005}. The dependence on geographic region is particularly troubling for implementation in climate models, which are used to model the atmosphere and make predictions about climates which may differ significantly from the current climate.

\cite{barker_1996} and \cite{oreopoulos_and_barker_1999} proposed other approaches to treat the subgrid heterogeneity which involved weighting the optical depths used in the radiative transfer equations by a distribution function. Their particular method has been referred to as the ``gamma-weighted two-stream approximation.'' \cite{oreopoulos_and_barker_1999} implemented this scheme in a large-scale model and found plane-parallel-homogeneous biases relative to 3D Monte Carlo and ICA calculations, but with increased computational cost (a factor of two or greater).

A method has been developed by \cite{shonk_and_hogan_2008} (``Tripleclouds'') that breaks each vertical level in the model into three regions: one for the clear-sky portion of each grid-box, one for the optically thin clouds and one for optically thick clouds in each grid-box. A probability distribution for cloud water is modelled or assumed, and selecting a value representing the lower percentile and a value representing the upper percentile of the distribution defines the two cloudy regions in each level. Selecting different percentiles has the effect of varying the amount of variability represented. This method has the advantage of being computationally efficient and has the potential to reduce plane-parallel biases by providing more information about the distribution of cloud water in a grid box (two values instead of one), but the full distribution of cloud water is not sampled. Also, to implement this scheme in a model would require modifications to the radiation code itself in order to handle the two cloudy regions in each level. Also, this may not extend naturally to implementation in subcolumn sampling codes used in instrument simulator diagnostics \citep{klein_and_jakob_1999}.

Perhaps the most straightforward and accurate way to account for subgrid-scale variability in cloud structure and properties would be to assume some probability distribution for each grid-box, assign values for various cloud properties at each level at a number of subcolumns in each grid-box by sampling from the probability distribution, and performing a full ICA calculation on the subcolumns. However, this would be extremely computationally expensive and intractable in large-scale models like those used to simulate global climate. \cite{pincus_et_al_2003} recognized that it is the double integral over both cloud and spectral space that makes this troublesome, and proposed selecting only a single subcolumn for each spectral band to perform the ICA calculation on, rather than performing the calculation for each spectral band on each subcolumn. Simultaneously sampling both cloud type and spectral space drastically increases the computational efficiency of performing an ICA-like integration, at the expense of introducing random but unbiased noise in the instantaneous fluxes due to the stochastic sampling. The Monte Carlo Independent Column Approximation (McICA), as it has been called, has been implemented in a number of next-generation climate models \citep[e.g.][]{cam5_description,donner_et_al_2011,von_salzen_et_al_2012}. This method lends itself naturally to a consistent and realistic treatment of subgrid-scale variability in both the radiative fluxes and in the cloud diagnostic outputs from instrument simulators, because the same subcolumns can be used as input to the instrument simulator code as is used by the radiation code.

\section{Proposed work}
The over-arching goal of this work would be to improve the representation of subgrid-scale variability in cloud structure and properties in large-scale models as seen by the model radiation, and to evaluate the impact this has on the domain averaged fluxes and heating rates, and on cloud diagnostics (i.e. simulator-enabled diagnostics) consistent with the radiation. This is partly motivated by the identification of potentially compensating errors in cloud radiative properties using instrument simulators \citep[e.g.][]{kay_et_al_2012,hillman_2012}, and the need to insure that not only TOA fluxes are reasonable in large-scale models, but but also the profiles of fluxes and heating rates, as compensating biases in these quantities could conceivably have impacts on climate feedbacks. The following subsections outline a strategy for approaching this problem.

\subsection{Assessing radiative biases in large-scale models}
\label{benchmark}
The first step in this project would be to assess existing biases in not only the TOA fluxes simulated by large-scale models, but also profiles of fluxes and heating rates. There have already been studies that address the impact of subgrid-scale variability on domain averaged fluxes \citep[e.g.][]{barker_et_al_1999}, but the goal here would be to look at large-scale models which we know neglect to treat this variability in a reasonable fashion and to take a critical look at biases in radiative profiles. This would provide a baseline evaluation for any improvements we make to the subgrid-scale variability of cloud structure and properties.

I think that the Community Atmosphere Model would make a good test-bed for this, as we already have contacts there willing to work with us, it is a widely used model under active development, and currently fails to treat the subgrid-scale variability in cloud properties with any sophistication. The newest version of CAM (CAM5) now uses RRTMG for radiative calculations. This code uses McICA, but CAM5 does not treat subgrid-scale variability in cloud optical properties. Cloud overlap is maximum-random, which has been shown by previous studies to be less than ideal and can also lead to biases in fluxes and heating rates \citep[e.g.][]{barker_et_al_1999}. We should expect then some large radiative biases from CAM5 (and previous versions of CAM) when looked at critically. Tuning efforts may eliminate much of the TOA flux biases, but biases in profiles of fluxes and heating rates will likely be large.

The question remains of how to actually perform this evaluation. One possibility would be to take a particular case study for evaluation, and compare fluxes and heating rate profiles from a cloud resolving model and from CAM5 both forced with the same large-scale dynamics. Different cases could be considered for different conditions. This could be supplemented with simulator diagnostics as well.

Another preliminary study that may be of interest is one assessing the sensitivity of fluxes, heating rates, and instrument simulator diagnostics from the McICA to variability in cloud optical properties. This could be a simple exercise in artificially introducing variability into the cloud optical properties on the McICA subcolumns, and noting any significant changes in the calculated fluxes, heating rates, and simulator diagnostics. This might be just a simple sanity check to perform before we go about developing an elaborate parameterization of subgrid-scale variability that we find in the end has no affect on the quantities we are interested in.

\subsection{Parameterizing subgrid-scale variability}
After performing a baseline assessment, the next and probably most difficult step is to develop or select a parameterization of subgrid-scale cloud variability. I think the most natural way to integrate a model for subgrid-scale variability into radiative schemes used in large-scale models is to take a PDF-based approach for use with the McICA. Such a method would presumably prescribe a PDF of cloud optical properties based on the large scale atmospheric state and the grid-box mean quantities. I believe such a scheme has already been implemented to an extent in the latest climate model produced by CCCma, the CanAM4 \citep{von_salzen_et_al_2012}, which assumes a gamma distribution for the cloud condensate based on previous studies such as that by \cite{barker_1996}. I have had some preliminary conversations with Jason Cole at CCCma about this, and he is interested in expanding upon their parameterization and further discussing possibilities with us for how to do this. I'm not completely clear on the details of the current implementation, and more discussions on this need to take place.

The aforementioned strategy may provide for a flexible implementation applicable to a range of climate models, and in particular may lend itself to development of an improved stand-alone subcolumn generator for COSP, but with recent interest in PDF-based cloud parameterizations it may make more sense to connect the subcolumns used by the radiative code to those used in the cloud physics parameterizations more directly. There is an on-going project to implement the Cloud Layers Unified By Binormals \citep[CLUBB][]{golaz_et_al_2002} into CAM, which would naturally provide the subgrid-scale variability we are after \citep{wood_2012}. Rob Wood expressed interest in discussing this further and keeping in touch on this front.

\subsection{Implementing in a large scale model}
The next step would be to implement the parameterization of subgrid-scale clouds into a large-scale model and to test the impact of the changes on the calculated fluxes and heating rates and the simulated cloud diagnostics. Again, I think CAM5 would make an excellent test-bed for this, since much of the machinery is already in place (McICA, and on-going development to include a PDF-based cloud scheme).

In the case of developing a new subcolumn generator that is somewhat independent of the cloud physics parameterization (i.e., the first method described in the previous subsection, not the integrated CLUBB method), the parameterization can be tested before being implemented into a particular large-scale model. This can be done by using fields generated by a cloud resolving model, and applying various averaging methods and radiative transfer solvers. One strategy would be to take a domain maybe the size of a typical climate model grid-box for a particular case. A benchmark calculation might be to run an ICA radiative transfer code on the fully resolved CRM fields. One test might consist of averaging the CRM fields over the horizontal domain, assigning subcolumns using the new subcolumn generator, and running the ICA code on the generated subcolumns and comparing the fluxes and heating rates with the ICA calculation on the fully resolved CRM fields. This would test the parameterization of subgrid-scale clouds. A further test may be to do a McICA calculation on the generated subcolumns, and compare to the two ICA calculations.

Next comes testing in a large-scale model. This would revisit the benchmark assessment described above in Section \ref{benchmark}, possibly comparing fluxes and heating rates and cloud instrument simulator diagnostics for selected case studies with those derived from cloud resolving models and evaluating reductions in biases identified in Section \ref{benchmark}.

\section{Outlook}
Subgrid-scale variability in cloud structure and cloud properties have been shown to be important to radiative fluxes and heating rate profiles, and neglecting this structure can lead to large biases in these calculated radiative quantities \citep{barker_et_al_1999}. The above outlines a strategy for improving the way in which large-scale models account for clouds and their variability in radiative calculations and in cloud instrument simulator diagnostics. Compensating biases in cloud properties have been identified in previous studies using these simulator-enabled diagnostics, and reducing these biases could have implications for how cloud feedbacks are simulated by climate models. In light of this, I think this is an important area of research which deserves more attention than it has been given in the past. I think that if I do end up going down this path for my Ph.D. work, some natural choices for committee members would be Tom Ackerman, Roger Marchand, Rob Wood, Qiang Fu, Chris Bretherton, and Jen Kay at NCAR if we end up working with the CAM. Important questions regarding the feasibility of this as a Ph.D. project include what work has already been done in this area, what work is currently being done by others, and what contribution I can make to scientific understanding in relation to this area of research. 
\bibliographystyle{ametsoc}
\bibliography{phd}
\end{document}

% Potential committee members:
% Thomas Ackerman:  Radiation, instrument simulators
% Roger Marchand:   Radiation, instrument simulators
% Rob Wood:         Cloud structure and properties
% Chris Bretherton: Cloud structure and properties, modelling
% Qiang Fu:         Radiation
% Jen Kay:          Modelling
