\chapter{Introduction}
\label{introduction_chapter}
Large-scale (global) models of the atmosphere and climate system are fundamental tools that aid in our understanding of climate system. They are used not only to study interactions between different components of the climate system, but also to perform simulations of future climate change relevant for informing government policy decisions. The formulation of these models are evaluated on multiple scales, from testing the individual components that go into the models (such as a particular physical process like convection) to evaluating the simulation of climate as a whole. On the large-scale, models are often evaluated by comparing simulations of present-day climate with observations of the present-day climate system. The sources for these observations are diverse, and depend on the particular aspect of the climate being evaluated.

Clouds are a critic piece of the climate system, and yet the simulation of clouds by global climate models (GCMs, also general circulation models) remains a challenge, and cloud feedback processes are well-known to be a primary source of uncertainty in projections of future climate \citep{cess_et_al_1990, bony_and_dufresne_2005, williams_and_webb_2009, medeiros_et_al_2008, dufresne_and_bony_2008, bony_et_al_2006}. This makes evaluation of clouds in large-scale models of utmost importance. 

Observational records of cloud occurrence and other properties from satellite imagers including the International Satellite Cloud Climatology Project \citep[ISCCP][]{rossow_and_schiffer_1999}, the Moderate Resolution Imaging Spectroradiometer \citep[MODIS][]{king_et_al_2003}, and the Multi-angle Imaging Spectroradiometer \citep[MISR][]{diner_et_al_2002, diner_et_al_2005} provide a natural baseline for the evaluation of the large-scale cloud statistics simulated by these models because they provide near-global coverage and an increasingly long time-series. Comparisons of this type have been used to evaluate models for as long as such observations have been available [citations], but comparisons between satellite-retrieved and modeled cloud properties are difficult because of fundamental differences between how clouds can be measured from space and how they are represented in large-scale models. These differences stem from both unavoidable limitations in the satellite retrieval process, as well as from limitations that arise due to the differences in scale between satellite retrievals and current GCMs. For example, cloud top height or cloud top pressure retrievals based on visible or infrared observations (e.g., ISCCP, MODIS, and MISR) are known to have significant problems when clouds with low amounts of condensate (i.e. non-opaque clouds or cloud-tops) are present, especially for scenes with multi-layer clouds where the upper layer cloud is optically thin \citep{marchand_et_al_2010, pincus_et_al_2012}. Fundamentally, the visible and infrared observations gathered by MODIS, MISR and ISSCP cannot fully constrain the vertical distribution of condensate, including discriminating between condensate types in differing layers, and this leads to uncertainties and systematic errors in the determination (retrieval) of cloud top height. Models, however, specify (or resolve) the vertical distribution of condensate to some degree. This fundamental difference between retrievals of cloud top height and the vertical distribution of clouds specified by a model makes any direct comparisons between the two somewhat ambiguous.
%One can therefore account for errors in the satellite retrieval process by ``simulating'' the cloud top height or cloud top pressure a satellite would observe given the model cloud field, and obtain a better evaluation of the model cloud-top by comparing the satellite simulated cloud top height with the satellite retrieval rather than comparing the true cloud top height from the model with the satellite retrieval.  

An alternative to this often ambiguous direct comparison between satellite-retrieved and modeled clouds is to first ``simulate'' the satellite view of clouds from the model-simulated atmospheric state. The goal with this approach is to account for the known errors in the satellite retrieval process by forward-modeling or emulating the retrieval technique used for a particular satellite instrument from the available model fields, with the goal of providing a description of what a given satellite instrument would see given the model-simulated atmosphere. These simulated or psuedo-retrievals are expected to be more directly comparable to the available satellite retrievals than the raw model fields, thus enabling a more appropriate evaluation of model clouds against satellite observations.

The ISCCP simulator introduced by \cite{klein_and_jakob_1999} has been widely used in model comparisons with ISCCP observations \citep{webb_et_al_2001, norris_and_weaver_2001, lin_and_zhang_2004, zhang_et_al_2005, wyant_et_al_2006, klein_et_al_2013}. The ISCCP simulator produces joint histograms of cloud top pressure and cloud optical depth from model fields that can be directly compared with joint histograms produced from ISCCP retrievals. In effect, each bin in the ISCCP histogram is a cloud fraction that quantifies how often clouds within a certain range of cloud top pressures and cloud optical depths occur, and with the sum of all bins yielding the total cloud fraction. Because outgoing longwave radiation is strongly influenced by cloud top height (and cloud amount) and outgoing shortwave radiation is strongly influenced by cloud optical depth (and cloud amount), comparisons using the ISCCP joint histograms provide an evaluation of model cloud amount that is linked to the impact of clouds on the model radiation budget. This is extremely useful for assigning radiative importance to diagnosed errors in cloud properties, but is also useful for exploring cloud feedbacks associated with future climate change. The latter point is demonstrated by \cite{zelinka_et_al_2012a} and \cite{zelinka_et_al_2012b}, who exploit this link to the radiation budget to introduce a new framework for calculating cloud feedbacks by creating a radiative ``kernel'' from the ISCCP histogram output by the ISCCP simulator that represents the change in the radiative forcing that results from changes in each of the ISCCP histogram components.  

The utility of the ISCCP simulator has inspired efforts to construct simulators for additional satellite-based imagers, including MISR \citep{marchand_and_ackerman_2010} and MODIS \citep{pincus_et_al_2012}. Additional simulators have also recently been developed for the CloudSat \citep{stephens_et_al_2002} cloud profiling radar \citep[Quickbeam;][]{haynes_et_al_2007}, and for the Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP) lidar \citep{chepfer_et_al_2008} onboard the Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observations \citep[CALIPSO][]{winker_et_al_2007} satellite. [comment on evalations using the individual simulators]

With the goal of facilitating the implementation of these simulators into global climate models, the Cloud Feedback Model Intercomparison Project (CFMIP; [citation]) has collected the ISCCP, MISR, MODIS, CloudSat, and CALIPSO simulators into a single software package with a common interface: the CMFIP Observation Simulator Package \citep[COSP;][]{bodas-salcedo_et_al_2011}. This has enabled both coordinated multi-model experiments comparing simulated cloud properties across models as well as innovative multi-sensor analyses of models \citep[e.g.,][]{bodas-salcedo_et_al_2011, kay_et_al_2012, klein_et_al_2013}, nominally leading to more robust evaluation of clouds in climate models.

While the goal of the simulator approach is to remove ambiguities in comparisons between models and remote sensing observations of clouds, not all ambiguities in model-to-observation comparisons can be removed with the simulator framework. The presence of remaining uncertainties or ambiguities in simulated and retrieved cloud properties that are unaccounted for or poorly represented by the simulators may undermine conclusions reached using this framework. It is therefore important to identify and understand the uncertainties and limitations of this framework in order to be able to confidently attribute differences between simulated and retrieved cloud properties unambiguously to model biases. 

As described by \cite{pincus_et_al_2012} and illustrated schematically by \cite{bodas-salcedo_et_al_2011} (see Figure 1 in \cite{bodas-salcedo_et_al_2011}, and also Figure \ref{simulator_schematic} here), simulating satellite retrievals from global model output is essentially a three-part process, involving 1) inferring pixel-scale cloud properties from the large-scale description provided by models, 2) simulating the pixel-scale satellite retrievals from the inferred pixel-scale (or subgrid-scale) cloud properties from the model, and finally 3) aggregating the simulated pixel-scale retrievals into statistical summaries consistent with the gridded, global summary products distributed by the satellite teams (often referred to as ``Level 3'' products in satellite retrieval nomenclature). In general, there can be errors associated with each of these three steps in the simulator process, and the primary goal of the present study is to identify and quantify these errors, and ultimately to present strategies for reducing these errors in order to enable more robust evaluation of models in the future.

\begin{figure}
	\centering
	\includegraphics{graphics/simulator_schematic.pdf}
	\caption{Schematic of the simulator framework.}
	\label{simulator_schematic}
\end{figure}

The first of these steps, inferring subgrid-scale cloud properties, is necessary because the resolution of typical global models is much coarser than the scales at which satellite retrievals are performed. As pointed out by \cite{pincus_et_al_2012}, these bulk statistics at the gridbox scale imply a distribution of possible retrievals within each gridbox, each resulting from a different possible combination of subgrid-scale profiles. This is due to the fact that simple profiles of averaged quantities at larger scales do not in themselves fully constrain the distribution of profiles at smaller scales, and simulating the satellite views of clouds depends on detailed knowledge of the overlapping nature of clouds at scales approximating satellite pixels. This is accounted for in the simulator framework by generating stochastic samples of ``subcolumn'' profiles, which reproduce the gridbox-averaged profiles in the limit of many samples and are consistent with some external assumption about how the cloudy parts of the gridbox overlap vertically \citep{klein_and_jakob_1999}. This problem is not unique to simulating satellite-retrieved quantities, but is also important for simulating radiative fluxes and heating rates within models as well. However, the assumptions made in the subcolumn sampling process, namely that cloud occurrence obeys a conceptually simple combination of maximum and random overlap and that cloud (and precipitation) condensate is horizontally homogeneous on the scale of model gridboxes, have recently been shown to lead to substantial errors in simulated radiative fluxes and heating rates in models \citep{barker_et_al_1999, oreopoulos_et_al_2012} [others?]. In Chapter \ref{subgrid1_chapter} here it is shown that these assumptions similarly lead to substantial errors in simulated satellite retrievals. In Chapter \ref{subgrid2_chapter} an improved framework for sampling these subcolumns is presented that better represents the subgrid-scale cloud and precipitation properties, and it is shown that these improvements can substantially reduce the errors identified in Chapter \ref{subgrid1_chapter}

Errors in the second step in the simulator framework (simulating the pixel-scale satellite retrievals), can arise due to incomplete or incorrect implementation of the retrieval process, even given perfect pixel-scale cloud properties as inputs. While every effort is made to build the simulators to account for as many features of the individual retrievals as possible, verification of the simulators is difficult, and documented verification is limited in the literature. The basic question that largely remains unanswered is, given perfect descriptions of the cloudy atmosphere as inputs to the simulators, are they able to faithfully reproduce the retrieved cloud properties that the instrument they attempt to simulate actually retrieves? A theoretical framework for answering this question is to supply actual profiles of cloud properties as inputs to the simulators, and then to compare the simulated retrievals with actual coincident retrievals. Using this framework to answer this question is difficult because it requires some source for these ``perfect inputs'' on which to run the simulators simultaneous with actual retrievals from the instruments. \cite{mace_et_al_2009} and \cite{mace_et_al_2011} use a multi-sensor approach using ground-based remote sensing retrievals of cloud properties to derive inputs to the ISCCP simulator, run the ISCCP simulator directly on these inputs and then compare the simulated ISCCP cloud properties with actual coincident ISCCP retrievals. While the input profiles derived from the ground-based retrievals are likely imperfect and have their own associated uncertainties themselves, studies such as these are important for building confidence in the fidelity of the simulator framework itself. In Chapter \ref{misr_chapter}, an evaluation of the MISR simulator is presented, using a conceptually similar framework to that used in \cite{mace_et_al_2009} and \cite{mace_et_al_2011}.

Following the quantification of uncertainties and errors in the simulator framework presented in Chapters \ref{subgrid1_chapter}, \ref{subgrid2_chapter}, and \ref{misr_chapter}, the simulator framework is applied to a comparison of cloud properties in a collection of models in Chapter \ref{cmip5_chapter}, with a specific focus on those differences between models and satellite retrievals that are beyond the range of uncertainties estimated in Chapters \ref{subgrid1_chapter}, \ref{subgrid2_chapter}, and \ref{misr_chapter}. The results presented in Chapter \ref{cmip5_chapter} serve to further illustrate the utility of the simulator framework, but more importantly the discussion therein serves to underscore the conclusions reached in the previous chapters and the importance of both understanding and reducing the sources of errors in model to observation comparisons.
